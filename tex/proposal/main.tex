\documentclass{article}
\usepackage[final]{nips_2017}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\title{Mathematical Problem Solving with Language Models (Natural Language)}

\author{
  %%John K.~Doe\thanks{Use footnote for providing further
  %%  information about author (webpage, alternative
  %%  address)---\emph{not} for acknowledging funding agencies.} \\
  %%Department of Computer Science\\
  %%Stanford University\\
  %%\texttt{***@stanford.edu} \\
  %% examples of more authors
  %% \And
  Yacine Dolivet\\
  \texttt{yacine@stanford.edu} \\
  \And
  Stephen Ge\\
  \texttt{scge@stanford.edu} \\
   \And
  Alexander Kuznetsov\\
  \texttt{skz@stanford.edu} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\begin{center}
\includegraphics[width=0.7cm, height=0.7cm]{stanford.png}
\end{center}

\maketitle

\section{Problem Description}	
We will attempt to improve the mathematical reasoning performance of small (<10B) LLMs. This is interesting because LLMs can produce text which seems logically and reasonably sound, but often contains mathematical or reasoning errors. Hence this is a valuable area of improvement.


\section{Project Challenges}
LLMs are large and expensive to train. Hence we are focusing on small LLMs and will start with open-source base models (e.g. LLaMa-7b). The area has been of recent research interest ranging from academia, the major research labs, to Kaggle competitions. There are various methods to draw inspiration from, but also a decent amount of already explored approaches.

\section{Datasets}
We will use existing datasets such as GSM8K \cite{cobbe}("8.5K high quality linguistically diverse grade school math word problems") and MATH \cite{hendrycks} ("12,500 challenging
competition mathematics problems"). This will also allow us to compute metrics we can compare against SOTA and other approaches. 

We will also investigate augmenting datasets with synthetic data, such as by using other models to change the values in existing datasets. Other related datasets in the literature that we may draw upon include DeepSeekMath Corpus \cite{shao}, MetaMathQA, NuminaMath, PRM800K \cite{lightman}, TinyGSM \cite{liu}, 

\section{Methods}
We will explore various approaches including
\begin{enumerate}
\item Synthetic data generation for additional training or tuning. Also dataset curation which has proven successful with small language models \cite{gunasekar} 
\item Finetuning with LoRA \cite{hu}
\item Training a verifier-like model with variations on the targeting the outcome, process, or other relevant parts of data to learn from
\item Intermediate steps/chain of thought \cite{wei} prompting style approach to elicit reasoning from the language model
\item Knowledge distillation from a larger model
\item Curriculum Learning (start with training on simpler problems)
\item Using external solvers or symbolic packages, training model to better utilize more exact computation. For example, python can be used for reliable numeric computation - which are a notorious weak point of LLMs - and we will explore the use of coq for verifying logical consistency
\end{enumerate}

We are interested in reinforcement learning related methods, however recognize the potential algorithmic complexity and compute requirements.


\section{Evaluation}
We plan to use performance on GSM8K and MATH as the primary evaluations. There are various existing approaches to improve performance, and those may be used as baselines or ablation studies in our project.

We will focus on comparing against other model performance within the 7b-13b class. 

Quantitatively, specific metrics we will inspect to assess performance include accuracy and pass@k. Accuracy is defined as the fraction of correct answers, while pass@k measures whether any of the top $k$ generated solutions solve each problem. Pass@1 and accuracy are likely equivalent, but assessing performance for larger values of $k$ may give us insight into how close the model is to solving problems and where it is going astray. 

Qualitatively, we will manually inspect a subset of responses for quality and consistency, especially intermediate chain-of-thoughts prompts to see how well the model is making progress towards solving problems.

\bibliographystyle{plain}
\bibliography{references}

\end{document}